# æ–‡ä»¶å: oil_prices_app_mobile_v3.py
"""
ç§»åŠ¨ç«¯å‹å¥½ç‰ˆï¼šåŸæ²¹ä»·æ ¼é¢æ¿
- æŠ˜çº¿å›¾å›ºå®šæ”¾åœ¨ "å†å²åŸæ²¹ä»·æ ¼èµ°åŠ¿ï¼ˆEIA æ•°æ®ï¼‰" æ ‡é¢˜ä¸‹æ–¹
- é¢‘ç‡é€‰æ‹©ä½¿ç”¨æ¨ªå‘ radioï¼ˆæ‰‹æœºç«¯ä¹Ÿä¸ºæ¨ªå‘å¸ƒå±€ï¼‰
- è¡¨æ ¼åˆ†é¡µä½¿ç”¨æ»‘å—é€‰æ‹©é¡µç ï¼ˆæ°´å¹³æ»‘å—ï¼Œæ‰‹æœºç«¯ä½“éªŒå¥½ï¼‰ï¼Œå¹¶ä¿ç•™ä¸Šä¸€é¡µ / ä¸‹ä¸€é¡µæŒ‰é’®
- ä¸‹è½½æŒ‰é’®åœ¨è¡¨æ ¼æ ‡é¢˜ä¸‹æ–¹
"""
import requests
import pandas as pd
import streamlit as st
import plotly.express as px
import os
import io
from datetime import datetime

# ========== é…ç½® ==========
API_KEY = "GOqcqu0QJ0yQvzYCCYB72exYzlEun88nwKAqXMQP"
SERIES = {
    "å¸ƒä¼¦ç‰¹åŸæ²¹": "PET.RBRTE.D",
    "WTIåŸæ²¹": "PET.RWTC.D"
}
DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)


# ========== å°å·¥å…· / æ•°æ®è·å– ==========
def _safe_float(s):
    try:
        return float(s)
    except Exception:
        return None


@st.cache_data(ttl=60)
def fetch_realtime_prices():
    endpoints = {
        "å¸ƒä¼¦ç‰¹åŸæ²¹æœŸè´§": "http://hq.sinajs.cn/list=hf_OIL",
        "WTIåŸæ²¹æœŸè´§": "http://hq.sinajs.cn/list=hf_CL",
    }
    headers = {"Referer": "https://finance.sina.com.cn", "User-Agent": "Mozilla/5.0"}
    out = {}
    for name, url in endpoints.items():
        price, ts = None, None
        try:
            r = requests.get(url, headers=headers, timeout=6)
            r.encoding = "gbk"
            if "=" in r.text:
                payload = r.text.split("=", 1)[1].strip().strip('";')
                parts = payload.split(",")
                if len(parts) >= 2 and _safe_float(parts[1]) is not None:
                    price = round(float(parts[1]), 2)
                else:
                    for p in parts[:6]:
                        val = _safe_float(p)
                        if val is not None:
                            price = round(val, 2)
                            break
                if parts and ":" in parts[-1]:
                    ts = parts[-1]
        except Exception:
            pass
        out[name] = {"price": price, "time": ts}
    return out


@st.cache_data(ttl=3600)
def fetch_eia_data_v2(series_id, api_key, start=None, end=None):
    url = f"https://api.eia.gov/v2/seriesid/{series_id}"
    params = {"api_key": api_key}
    if start: params["start"] = start
    if end: params["end"] = end
    resp = requests.get(url, params=params, timeout=15)
    data = resp.json()
    if "response" in data and "data" in data["response"]:
        records = data["response"]["data"]
        df = pd.DataFrame(records)[["period", "value"]].rename(columns={"period": "æ—¥æœŸ", "value": "ä»·æ ¼"})
        df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"])
        return df.sort_values("æ—¥æœŸ")
    else:
        return pd.DataFrame(columns=["æ—¥æœŸ", "ä»·æ ¼"])


def load_or_update_data(series_id, name):
    csv_path = os.path.join(DATA_DIR, f"{name}.csv")
    if os.path.exists(csv_path):
        df_local = pd.read_csv(csv_path, parse_dates=["æ—¥æœŸ"])
        last_date = df_local["æ—¥æœŸ"].max().strftime("%Y-%m-%d")
    else:
        df_local = pd.DataFrame(columns=["æ—¥æœŸ", "ä»·æ ¼"])
        last_date = None
    df_new = fetch_eia_data_v2(series_id, API_KEY, start=last_date)
    if df_new is not None and not df_new.empty:
        df_all = pd.concat([df_local, df_new]).drop_duplicates(subset="æ—¥æœŸ").sort_values("æ—¥æœŸ").reset_index(drop=True)
        df_all.to_csv(csv_path, index=False)
        return df_all
    else:
        return df_local


def aggregate_data(df, freq):
    """è¿”å›æŒ‰ freq èšåˆåçš„ DataFrameï¼Œ'æ—¥æœŸ' ä¸º datetimeï¼Œ'ä»·æ ¼' ä¸ºæ•°å€¼"""
    if df is None or df.empty:
        return pd.DataFrame(columns=["æ—¥æœŸ", "ä»·æ ¼"])
    df = df.copy()
    if freq == "æ—¥":
        return df.sort_values("æ—¥æœŸ").reset_index(drop=True)
    elif freq == "æœˆ":
        df2 = df.resample("M", on="æ—¥æœŸ").mean().reset_index()
        return df2.sort_values("æ—¥æœŸ").reset_index(drop=True)
    elif freq == "å¹´":
        df2 = df.resample("Y", on="æ—¥æœŸ").mean().reset_index()
        return df2.sort_values("æ—¥æœŸ").reset_index(drop=True)
    else:
        return df.sort_values("æ—¥æœŸ").reset_index(drop=True)


# ========== UI è¾…åŠ©ï¼šCSS å’Œ åˆ†é¡µè¡¨æ ¼ ==========
def _inject_css():
    st.markdown(
        """
        <style>
        /* å…¨å±€æŒ‰é’®åœ†è§’ä¸é˜´å½± */
        .stButton>button, div[data-testid="stDownloadButton"] button {
            border-radius: 10px;
            padding: 8px 12px;
            font-weight: 600;
            box-shadow: 0 2px 8px rgba(0,0,0,0.08);
            border: none;
        }
        /* ç¾åŒ–ä¸‹è½½æŒ‰é’®æ›´é†’ç›® */
        div[data-testid="stDownloadButton"] button { font-weight:700; }
        /* è¡¨æ ¼æ ·å¼ï¼ˆé€‚é…æ‰‹æœºå¤§å­—ä½“ï¼‰ */
        .big-table table { font-size:4.4vw; width:100%; border-collapse:collapse; }
        .big-table th, .big-table td { padding:8px 10px; text-align:center; border:1px solid #eee; }
        .big-table th { background:#fafafa; font-weight:700; }
        /* é¡µç ä¿¡æ¯ */
        .page-info { text-align:center; margin:6px 0 8px 0; font-size:0.95rem; color:#333; }
        /* ä¿è¯ radio æ¨ªå‘åœ¨çª„å±ä¹Ÿæ˜¾ç¤ºä¸ºæ¨ªå‘ï¼ˆst.radio horizontal=True å·²æœ‰ï¼Œä½†è¿™é‡Œåšè¡¥å¼ºï¼‰ */
        .stRadio > div[role="radiogroup"] { display:flex; flex-wrap:nowrap; gap:6px; }
        /* é˜²æ­¢åˆ—åœ¨è¶…çª„å±ä¸‹æŠ˜å å¾—å¤ªéš¾çœ‹ï¼Œå…è®¸æ¨ªå‘æ»šåŠ¨ */
        .horizontal-scroll { overflow-x:auto; white-space:nowrap; }
        </style>
        """,
        unsafe_allow_html=True
    )


def show_df_centered(display_df, page_size=15, slider_key="page_slider"):
    """
    display_df: pandas.DataFrame (æ—¥æœŸ å­—æ®µåº”å·²ä¸ºå­—ç¬¦ä¸²æˆ–å¯æ˜¾ç¤º)
    page_size: æ¯é¡µè¡Œæ•°
    slider_key: ç”¨äº slider çš„ session_state keyï¼ˆæ”¯æŒå¤šä¸ªè¡¨æ—¶ä¼ ä¸åŒ keyï¼‰
    """
    total_rows = len(display_df)
    if total_rows == 0:
        st.info("å½“å‰æ²¡æœ‰å¯æ˜¾ç¤ºçš„æ•°æ®ã€‚")
        return

    total_pages = max(1, (total_rows - 1) // page_size + 1)

    # åˆå§‹åŒ– page_num
    if "page_num" not in st.session_state:
        st.session_state.page_num = 1
    if st.session_state.page_num < 1:
        st.session_state.page_num = 1
    if st.session_state.page_num > total_pages:
        st.session_state.page_num = total_pages

    # é¡µç æ»‘å—ï¼ˆæ°´å¹³æ§ä»¶ï¼Œç§»åŠ¨ç«¯å‹å¥½ï¼‰
    # ä½¿ç”¨ç‹¬ç«‹ slider_key ä¿è¯å¤šè¡¨æˆ–å¤šå¤„ä½¿ç”¨ä¸ä¼šå†²çª
    st.markdown("<div class='horizontal-scroll'>", unsafe_allow_html=True)
    new_page = st.slider("è·³è½¬é¡µç ", min_value=1, max_value=total_pages, value=st.session_state.page_num, key=slider_key)
    st.markdown("</div>", unsafe_allow_html=True)

    # å¦‚æœæ»‘å—æ”¹å˜ï¼Œåˆ™æ›´æ–° page_num
    st.session_state.page_num = int(new_page)

    # åŒä¸€è¡Œæ˜¾ç¤º ä¸Šä¸€é¡µ / é¡µç ä¿¡æ¯ / ä¸‹ä¸€é¡µï¼ˆè‹¥ç©ºé—´ä¸è¶³ä¼šæ¢è¡Œï¼Œä½†æ»‘å—ä¿è¯äº†æ¨ªå‘äº¤äº’ï¼‰
    c1, c2, c3 = st.columns([1, 2, 1])
    with c1:
        if st.button("â¬…ï¸ ä¸Šä¸€é¡µ"):
            if st.session_state.page_num > 1:
                st.session_state.page_num -= 1
                st.session_state[slider_key] = st.session_state.page_num
    with c2:
        st.markdown(f"<div class='page-info'>ç¬¬ {st.session_state.page_num} / {total_pages} é¡µ â€” å…± {total_rows} è¡Œ</div>", unsafe_allow_html=True)
    with c3:
        if st.button("ä¸‹ä¸€é¡µ â¡ï¸"):
            if st.session_state.page_num < total_pages:
                st.session_state.page_num += 1
                st.session_state[slider_key] = st.session_state.page_num

    # è®¡ç®—å½“å‰é¡µçš„æ•°æ®å¹¶æ˜¾ç¤º
    start = (st.session_state.page_num - 1) * page_size
    end = start + page_size
    page_df = display_df.iloc[start:end].reset_index(drop=True)

    html = page_df.to_html(index=False)
    st.markdown(f"<div class='big-table'>{html}</div>", unsafe_allow_html=True)


# ========== Streamlit é¡µé¢å¸ƒå±€ ==========
st.set_page_config(page_title="åŸæ²¹ä»·æ ¼ï¼šå®æ—¶æœŸè´§ & ç°è´§å†å²", layout="centered")
_inject_css()

st.title("ğŸ“Š å›½é™…åŸæ²¹ï¼šæœŸè´§å®æ—¶ & ç°è´§å†å²ä»·æ ¼é¢æ¿")

# â€”â€” æœŸè´§å®æ—¶ä»·æ ¼ â€”â€”
st.subheader("â± æœŸè´§å®æ—¶ä»·æ ¼")
rt = fetch_realtime_prices()
now_ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

for name, color in zip(["å¸ƒä¼¦ç‰¹åŸæ²¹æœŸè´§", "WTIåŸæ²¹æœŸè´§"], ["#FFC107", "#F44336"]):
    price = rt.get(name, {}).get("price")
    ts = rt.get(name, {}).get("time") or now_ts
    st.markdown(
        f"""
        <div style='background-color:{color};padding:12px;border-radius:12px;margin-bottom:10px;text-align:center;'>
            <h3 style='color:white;font-size:5vw;margin:6px 0;'>â›½ {name}</h3>
            <p style='color:white;font-size:6.5vw;font-weight:bold;margin:4px 0;'>{('--' if price is None else f'{price:.2f}')} ç¾å…ƒ/æ¡¶</p>
            <p style='color:white;font-size:3.5vw;margin:2px 0;'>æ›´æ–°æ—¶é—´ {ts}</p>
        </div>
        """, unsafe_allow_html=True
    )

st.markdown("---")

# â€”â€” ç°è´§æœ€æ–°ç»“ç®—ä»· â€”â€”
st.subheader("ğŸ†• ç°è´§æœ€æ–°ç»“ç®—ä»·")
raw_dfs = {}
spot_latest = {}
for name, sid in SERIES.items():
    df_raw = load_or_update_data(sid, name)
    raw_dfs[name] = df_raw
    if not df_raw.empty:
        last_row = df_raw.sort_values("æ—¥æœŸ").iloc[-1]
        spot_latest[name] = {"date": last_row["æ—¥æœŸ"].strftime("%Y-%m-%d"), "price": float(last_row["ä»·æ ¼"])}

for name, color in zip(["å¸ƒä¼¦ç‰¹åŸæ²¹", "WTIåŸæ²¹"], ["#FFC107", "#F44336"]):
    d = spot_latest.get(name)
    st.markdown(
        f"""
        <div style='background-color:{color};padding:12px;border-radius:12px;margin-bottom:10px;text-align:center;'>
            <h3 style='color:white;font-size:5vw;margin:6px 0;'>ğŸ›¢ {name}</h3>
            <p style='color:white;font-size:6.5vw;font-weight:bold;margin:4px 0;'>{('--' if d is None else f'{d["price"]:.2f}')} ç¾å…ƒ/æ¡¶</p>
            <p style='color:white;font-size:3.5vw;margin:2px 0;'>ç»“ç®—æ—¥æœŸ {('--' if d is None else d["date"])}</p>
        </div>
        """, unsafe_allow_html=True
    )

st.markdown("---")

# â€”â€” å†å²ä»·æ ¼ï¼ˆæŠ˜çº¿å›¾å›ºå®šåœ¨æ­¤æ ‡é¢˜ä¸‹æ–¹ï¼‰ â€”â€”
st.subheader("ğŸ“ˆ å†å²åŸæ²¹ä»·æ ¼èµ°åŠ¿ï¼ˆEIA æ•°æ®ï¼‰")

# å¹´ä»½èŒƒå›´é€‰æ‹©ï¼ˆæ”¾åœ¨å›¾è¡¨ä¸Šæ–¹ï¼‰
years = st.select_slider("é€‰æ‹©å±•ç¤ºçš„å¹´ä»½èŒƒå›´", options=list(range(2000, 2026)), value=(2015, 2025))

# ä¿è¯ freq æœ‰åˆå§‹å€¼
if "freq" not in st.session_state:
    st.session_state.freq = "æœˆ"

# è¿‡æ»¤æŒ‰å¹´ä»½
dfs = {}
for name, df_raw in raw_dfs.items():
    if not df_raw.empty:
        df = df_raw[(df_raw["æ—¥æœŸ"].dt.year >= years[0]) & (df_raw["æ—¥æœŸ"].dt.year <= years[1])]
        dfs[name] = df

# ä»…å½“ä¸¤ç»„æ•°æ®éƒ½æœ‰æ—¶æ˜¾ç¤ºå›¾ä¸è¡¨
if "å¸ƒä¼¦ç‰¹åŸæ²¹" in dfs and "WTIåŸæ²¹" in dfs and not dfs["å¸ƒä¼¦ç‰¹åŸæ²¹"].empty and not dfs["WTIåŸæ²¹"].empty:
    # å…ˆèšåˆä¸ºå½“å‰ freqï¼Œç”¨äºå›¾è¡¨ï¼ˆä¿æŒæ—¶é—´åˆ—ä¸º datetimeï¼‰
    df_b_plot = aggregate_data(dfs["å¸ƒä¼¦ç‰¹åŸæ²¹"], st.session_state.freq).rename(columns={"ä»·æ ¼": "å¸ƒä¼¦ç‰¹ä»·æ ¼"})
    df_w_plot = aggregate_data(dfs["WTIåŸæ²¹"], st.session_state.freq).rename(columns={"ä»·æ ¼": "WTIä»·æ ¼"})
    merged_for_plot = pd.merge(df_b_plot, df_w_plot, on="æ—¥æœŸ", how="outer").sort_values("æ—¥æœŸ").reset_index(drop=True)

    # æŠ˜çº¿å›¾ â€” ç›´æ¥æ”¾åœ¨ "å†å²åŸæ²¹ä»·æ ¼èµ°åŠ¿ï¼ˆEIA æ•°æ®ï¼‰" æ ‡é¢˜ä¸‹æ–¹
    fig = px.line(
        merged_for_plot,
        x="æ—¥æœŸ",
        y=["å¸ƒä¼¦ç‰¹ä»·æ ¼", "WTIä»·æ ¼"],
        labels={"value": "ä»·æ ¼ (ç¾å…ƒ/æ¡¶)", "æ—¥æœŸ": "æ—¥æœŸ"},
        title=f"å¸ƒä¼¦ç‰¹ vs WTI å†å²ä»·æ ¼è¶‹åŠ¿ï¼ˆ{st.session_state.freq}ï¼‰"
    )
    fig.update_layout(
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        margin=dict(l=10, r=10, t=40, b=10)
    )
    st.plotly_chart(fig, use_container_width=True, config={'responsive': True})

    st.markdown("---")

    # â€”â€” å›½é™…åŸæ²¹å†å²ä»·æ ¼è¡¨ï¼ˆé¢‘ç‡é€‰æ‹©æ”¾åœ¨æ­¤æ ‡é¢˜ä¸‹æ–¹ï¼‰ â€”â€”
    st.subheader("ğŸ“‹ å›½é™…åŸæ²¹å†å²ä»·æ ¼è¡¨")

    # æ¨ªå‘é¢‘ç‡é€‰æ‹©ï¼ˆä½¿ç”¨ st.radio horizontal=Trueï¼Œç§»åŠ¨ç«¯ä¼šæ¨ªå‘æ˜¾ç¤ºï¼‰
    freq = st.radio(
        "é€‰æ‹©æ•°æ®é¢‘ç‡",
        options=["æ—¥", "æœˆ", "å¹´"],
        index={"æ—¥": 0, "æœˆ": 1, "å¹´": 2}.get(st.session_state.freq, 1),
        horizontal=True,
        key="freq_radio"
    )
    # å°† radio çš„é€‰æ‹©å†™å› session_state.freqï¼ˆä¿æŒå…¼å®¹ï¼‰
    st.session_state.freq = freq
    # æ¯æ¬¡åˆ‡æ¢é¢‘ç‡å›åˆ°ç¬¬1é¡µ
    st.session_state.page_num = 1
    # é¡µé¢æ¯é¡µè¡Œæ•°ï¼ˆå¯æ”¹æˆä¸‹æ‹‰ï¼‰
    PAGE_SIZE = 15

    # é‡æ–°ä¸ºè¡¨æ ¼èšåˆï¼ˆåŸºäºæœ€æ–° freqï¼‰
    df_b = aggregate_data(dfs["å¸ƒä¼¦ç‰¹åŸæ²¹"], st.session_state.freq).rename(columns={"ä»·æ ¼": "å¸ƒä¼¦ç‰¹ä»·æ ¼"})
    df_w = aggregate_data(dfs["WTIåŸæ²¹"], st.session_state.freq).rename(columns={"ä»·æ ¼": "WTIä»·æ ¼"})
    merged = pd.merge(df_b, df_w, on="æ—¥æœŸ", how="outer").sort_values("æ—¥æœŸ", ascending=False).reset_index(drop=True)

    # ä¸ºæ˜¾ç¤ºæ ¼å¼åŒ–æ—¥æœŸï¼ˆç”¨äºè¡¨æ ¼ HTMLï¼‰
    display_df = merged[["æ—¥æœŸ", "å¸ƒä¼¦ç‰¹ä»·æ ¼", "WTIä»·æ ¼"]].copy()
    if not display_df.empty:
        if st.session_state.freq == "æ—¥":
            display_df["æ—¥æœŸ"] = display_df["æ—¥æœŸ"].dt.strftime("%Y-%m-%d")
        elif st.session_state.freq == "æœˆ":
            display_df["æ—¥æœŸ"] = display_df["æ—¥æœŸ"].dt.strftime("%Y-%m")
        elif st.session_state.freq == "å¹´":
            display_df["æ—¥æœŸ"] = display_df["æ—¥æœŸ"].dt.strftime("%Y")

    # ä¸‹è½½æŒ‰é’®æ”¾åœ¨è¡¨æ ¼æ ‡é¢˜ä¸‹æ–¹
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
        merged_download = merged.sort_values("æ—¥æœŸ")
        merged_download.to_excel(writer, index=False, sheet_name="åŸæ²¹ä»·æ ¼")
    st.download_button(
        label="ğŸ’¾ ä¸‹è½½æ•°æ®ä¸º Excel",
        data=output.getvalue(),
        file_name="åŸæ²¹ä»·æ ¼.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )

    # ç¿»é¡µè¡¨æ ¼ï¼ˆä½¿ç”¨æ°´å¹³æ»‘å—å’Œä¸Šä¸€/ä¸‹ä¸€æŒ‰é’®ï¼‰
    # ä½¿ç”¨å”¯ä¸€ slider key ä»¥é¿å…ä¸å…¶ä»–åœ°æ–¹å†²çª
    show_df_centered(display_df, page_size=PAGE_SIZE, slider_key="page_slider_main")

else:
    st.warning("æœªèƒ½æˆåŠŸè·å–å†å²æ•°æ®ï¼Œè¯·æ£€æŸ¥ API Key æˆ–ç½‘ç»œè¿æ¥ã€‚")
